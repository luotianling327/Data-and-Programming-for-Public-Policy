{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Anlysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install re\n",
    "# !pip install kaggle\n",
    "# !pip install gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import gensim.downloader\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, Dense, MaxPooling1D, GlobalMaxPooling1D, Flatten, Input, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'D:\\uchi\\2021Fall\\PPHA30536_Data and Programming for Public Policy II\\final-project-tianling-luo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(Can skip this step if one do not want to use kaggle API)\n",
    "If using kaggle API, please uncomment the following 4 lines\n",
    "\"\"\"\n",
    "# import kaggle\n",
    "# kaggle.api.authenticate()\n",
    "# kaggle.api.dataset_download_files('lakshmi25npathi/imdb-dataset-of-50k-movie-reviews', \n",
    "#                                   path=os.path.join(PATH, 'Data'), unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv file\n",
    "def read_csv(fname):\n",
    "    df = pd.read_csv(os.path.join(PATH, 'Data', fname))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label the reviews by 1 and 0\n",
    "def label_sentiment(df):\n",
    "    df['label'] = 0\n",
    "    for ind in df.index:\n",
    "        if df.loc[ind, ['sentiment']].values[0] == 'positive':\n",
    "            df.loc[ind, ['label']] = 1\n",
    "        if df.loc[ind, ['sentiment']].values[0] == 'negative':\n",
    "            df.loc[ind, ['label']] = 0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove undesired signs and characters\n",
    "def clean_text(sen):\n",
    "    # remove html tags\n",
    "    re.sub(r'<[^>]+>', ' ', sen)\n",
    "    # remove punctuations and numbers\n",
    "    sen = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "    # single character removal\n",
    "    sen = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sen)\n",
    "    # remove multiple spaces\n",
    "    sen = re.sub(r'\\s+', ' ', sen)   \n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change capital letters to lower ones\n",
    "def lower_token(token): \n",
    "    lower_token = []\n",
    "    for word in token:\n",
    "        lower_token.append(word.lower())\n",
    "    return lower_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "def remove_stopwords(token): \n",
    "    stoplist = stopwords.words('english')\n",
    "    filtered_token = []\n",
    "    for word in token:\n",
    "        if word not in stoplist:\n",
    "            filtered_token.append(word)\n",
    "    return filtered_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_clean_text(column_name, df):\n",
    "    cleaned_column = column_name + '_clean'\n",
    "    final_column = column_name + '_final'\n",
    "    \n",
    "    df[cleaned_column] = df[column_name].apply(lambda sen: clean_text(sen))\n",
    "    # transfer sentences to tokens\n",
    "    tokens = [word_tokenize(sen) for sen in df[cleaned_column]]\n",
    "    # change tokens to lower letters\n",
    "    lower_tokens = [lower_token(token) for token in tokens] \n",
    "    # remove stopwords\n",
    "    filtered_tokens = [remove_stopwords(token) for token in lower_tokens]\n",
    "    # combine results\n",
    "    result = [' '.join(token) for token in filtered_tokens] \n",
    "    df[final_column] = result\n",
    "    df['tokens'] = filtered_tokens\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you fail to download the file from kaggle, you can directly use this one.\n",
    "movie_reviews = read_csv(\"IMDB Dataset.csv\")\n",
    "# label the reviews by 0 and 1\n",
    "movie_reviews = label_sentiment(movie_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the texts\n",
    "movie_reviews = overall_clean_text('review', movie_reviews)\n",
    "movie_reviews = movie_reviews[['review_final', 'tokens', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building up CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get x and y data for training and testing\n",
    "def get_x_y(t_x, t_data, maxlen):\n",
    "    t_sequences = tokenizer.texts_to_sequences(t_x.tolist())\n",
    "    x = pad_sequences(t_sequences, maxlen=maxlen)\n",
    "    y = t_data['label'].values\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for embeddings\n",
    "def embeddings(tokens_list, word2vec, dim=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(dim)   \n",
    "    vec = [word2vec[word] if word in word2vec else np.random.rand(dim) for word in tokens_list]\n",
    "    average = len(vec) / np.sum(vec, axis=0)\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN architechture\n",
    "def cnn_construct(vocab_size, dim, embedding_matrix, maxlen):\n",
    "    input_data = Input(shape=(maxlen,))\n",
    "    embedding_layer = Embedding(vocab_size, \n",
    "                                dim, \n",
    "                                weights=[embedding_matrix], \n",
    "                                input_length=maxlen, \n",
    "                                trainable=False)\n",
    "    embedding = embedding_layer(input_data)\n",
    "    conv1 = Conv1D(filters=64, kernel_size=5, activation='relu')(embedding)\n",
    "    pool1 = MaxPooling1D(2)(conv1)\n",
    "    conv2 = Conv1D(filters=64, kernel_size=5, activation='relu')(pool1)\n",
    "    pool2 = GlobalMaxPooling1D()(conv2)\n",
    "    dropout = Dropout(0.5)(pool2)\n",
    "    dense1 = Dense(64, activation='relu')(dropout)\n",
    "    pred = Dense(1, activation='sigmoid')(dense1)\n",
    "    \n",
    "    model = Model(input_data, pred)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test data\n",
    "training_data, testing_data = train_test_split(movie_reviews, test_size=0.2)\n",
    "training_x = training_data['review_final']\n",
    "testing_x = testing_data['review_final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTICE: This step may take a bit long since the vector file is large.\n",
    "If the storage is not enough for this download, you might directly use the result in sentiment.xlsx data.\n",
    "\"\"\"\n",
    "word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = list(training_data['tokens'].apply(lambda x: embeddings(x, word2vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "maxlen = 100\n",
    "dim = 300\n",
    "\n",
    "training_words = [word for tokens in training_data[\"tokens\"] for word in tokens]\n",
    "training_wordlist = sorted(list(set(training_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and padding\n",
    "tokenizer = Tokenizer(num_words=len(training_wordlist), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(training_x.tolist())\n",
    "train_x, train_y = get_x_y(training_x, training_data, maxlen)\n",
    "test_x, test_y = get_x_y(testing_x, testing_data, maxlen)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, dim))\n",
    "for word, index in word_index.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[index,:] = word2vec[word]\n",
    "    else:\n",
    "        np.random.rand(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 300)          27153300  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 96, 64)            96064     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 48, 64)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 44, 64)            20544     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 64)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,274,133\n",
      "Trainable params: 120,833\n",
      "Non-trainable params: 27,153,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# construct CNN model\n",
    "model = cnn_construct(vocab_size, dim, embedding_matrix, maxlen)\n",
    "callbacks_list = [EarlyStopping(monitor='var_loss', min_delta=0.01, patience=4, verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "532/532 [==============================] - 48s 82ms/step - loss: 0.4447 - acc: 0.7876 - val_loss: 0.3996 - val_acc: 0.8197\n",
      "Epoch 2/16\n",
      "532/532 [==============================] - 37s 70ms/step - loss: 0.3349 - acc: 0.8591 - val_loss: 0.3304 - val_acc: 0.8567\n",
      "Epoch 3/16\n",
      "532/532 [==============================] - 36s 68ms/step - loss: 0.2980 - acc: 0.8769 - val_loss: 0.3600 - val_acc: 0.8453\n",
      "Epoch 4/16\n",
      "532/532 [==============================] - 36s 68ms/step - loss: 0.2561 - acc: 0.8961 - val_loss: 0.3337 - val_acc: 0.8595\n",
      "Epoch 5/16\n",
      "532/532 [==============================] - 36s 67ms/step - loss: 0.2160 - acc: 0.9135 - val_loss: 0.3707 - val_acc: 0.8477\n",
      "Epoch 6/16\n",
      "532/532 [==============================] - 37s 69ms/step - loss: 0.1858 - acc: 0.9262 - val_loss: 0.3519 - val_acc: 0.8593\n",
      "Epoch 7/16\n",
      "532/532 [==============================] - 32s 60ms/step - loss: 0.1527 - acc: 0.9407 - val_loss: 0.4118 - val_acc: 0.8608\n",
      "Epoch 8/16\n",
      "532/532 [==============================] - 32s 60ms/step - loss: 0.1209 - acc: 0.9548 - val_loss: 0.4339 - val_acc: 0.8573\n",
      "Epoch 9/16\n",
      "532/532 [==============================] - 32s 61ms/step - loss: 0.1076 - acc: 0.9595 - val_loss: 0.4653 - val_acc: 0.8580\n",
      "Epoch 10/16\n",
      "532/532 [==============================] - 32s 61ms/step - loss: 0.0885 - acc: 0.9660 - val_loss: 0.5009 - val_acc: 0.8528\n",
      "Epoch 11/16\n",
      "532/532 [==============================] - 34s 63ms/step - loss: 0.0762 - acc: 0.9715 - val_loss: 0.6149 - val_acc: 0.8468\n",
      "Epoch 12/16\n",
      "532/532 [==============================] - 33s 62ms/step - loss: 0.0760 - acc: 0.9712 - val_loss: 0.5015 - val_acc: 0.8512\n",
      "Epoch 13/16\n",
      "532/532 [==============================] - 33s 62ms/step - loss: 0.0613 - acc: 0.9768 - val_loss: 0.6505 - val_acc: 0.8502\n",
      "Epoch 14/16\n",
      "532/532 [==============================] - 32s 61ms/step - loss: 0.0595 - acc: 0.9777 - val_loss: 0.6636 - val_acc: 0.8525\n",
      "Epoch 15/16\n",
      "532/532 [==============================] - 32s 61ms/step - loss: 0.0525 - acc: 0.9807 - val_loss: 0.6509 - val_acc: 0.8518\n",
      "Epoch 16/16\n",
      "532/532 [==============================] - 32s 60ms/step - loss: 0.0512 - acc: 0.9808 - val_loss: 0.6891 - val_acc: 0.8547\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "epochs = 16\n",
    "batch_size = 64\n",
    "hist = model.fit(train_x, train_y, epochs=epochs, verbose=1, validation_split=0.15, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 12ms/step - loss: 0.7096 - acc: 0.8472\n",
      "Accuracy is:  0.8471999764442444\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "evaluation = model.evaluate(test_x, test_y, verbose=1)\n",
    "accuracy = evaluation[1]\n",
    "print(\"Accuracy is: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the SOTU texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer texts to lists of sentences\n",
    "def txt_to_sents(txts):\n",
    "    text_sents = []\n",
    "    text_sents_num = []\n",
    "    for num in range(len(txts)):\n",
    "        filename = os.path.join(PATH, 'Data\\SOTU texts', txts[num])\n",
    "        text_file = open(filename,'r',encoding='UTF-8-sig', errors='ignore')\n",
    "        text = text_file.read()\n",
    "        text = text.replace('\\n','')\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        doc = nlp(text)\n",
    "        sents = list(doc.sents)\n",
    "        text_sents.append(sents)\n",
    "        text_sents_num.append(len(sents))\n",
    "    return text_sents, text_sents_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer text sentences into list of DataFrames\n",
    "def df_list(num_txts, text_sents):\n",
    "    df_text = []\n",
    "    for num in range(num_txts):\n",
    "        text_sents[num]  = [sen.text for sen in text_sents[num]]\n",
    "        df = pd.DataFrame(text_sents[num])\n",
    "        df.columns = ['address']\n",
    "        df_text.append(df)\n",
    "    return df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare address texts for CNN prediction\n",
    "def prep_address(df, maxlen):\n",
    "    df = overall_clean_text('address', df)\n",
    "    address = df['address_final']\n",
    "    address_sequences = tokenizer.texts_to_sequences(address.tolist())\n",
    "    address_x = pad_sequences(address_sequences, maxlen=maxlen)\n",
    "    return address_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions through trained CNN\n",
    "def get_predictions(address_x, batch_size):\n",
    "    # predict sentiments\n",
    "    predictions = model.predict(address_x, batch_size=batch_size, verbose=1)\n",
    "    prediction_labels = []\n",
    "    for p in predictions:\n",
    "        if p[0] < 0.5:\n",
    "            prediction_labels.append(0)\n",
    "        else:\n",
    "            prediction_labels.append(1)\n",
    "    return prediction_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts of sotu address\n",
    "txts = os.listdir(PATH + '\\Data\\SOTU texts')\n",
    "num_txts = len(txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the whole text into sentences\n",
    "text_sents, text_sents_num = txt_to_sents(txts)\n",
    "# transfer each text to a DataFrame and get a list of DataFrames\n",
    "df_text = df_list(num_txts, text_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 259ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 227ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 227ms/step\n",
      "1/1 [==============================] - 0s 246ms/step\n",
      "1/1 [==============================] - 0s 242ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for df in df_text:\n",
    "    # prepare address texts\n",
    "    address_x = prep_address(df, maxlen)\n",
    "    # get predictions\n",
    "    prediction_labels = get_predictions(address_x, 1024)\n",
    "    # get a sentiment score of the whole address\n",
    "    score = sum(prediction_labels) / len(prediction_labels)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7534246575342466,\n",
       " 0.762589928057554,\n",
       " 0.8846153846153846,\n",
       " 0.8349514563106796,\n",
       " 0.783625730994152,\n",
       " 0.7757575757575758,\n",
       " 0.8016528925619835,\n",
       " 0.8783783783783784,\n",
       " 0.9122807017543859,\n",
       " 0.9285714285714286,\n",
       " 0.8537414965986394,\n",
       " 0.8290909090909091,\n",
       " 0.8415300546448088,\n",
       " 0.9102564102564102,\n",
       " 0.9053497942386831,\n",
       " 0.8711864406779661,\n",
       " 0.8542274052478134,\n",
       " 0.8794326241134752,\n",
       " 0.8571428571428571,\n",
       " 0.9042553191489362,\n",
       " 0.8685258964143426,\n",
       " 0.8249027237354085,\n",
       " 0.7952755905511811,\n",
       " 0.785,\n",
       " 0.8759398496240601,\n",
       " 0.8356164383561644,\n",
       " 0.7890625,\n",
       " 0.9105058365758755,\n",
       " 0.8991935483870968,\n",
       " 0.8911174785100286,\n",
       " 0.8899082568807339,\n",
       " 0.8697916666666666,\n",
       " 0.8994708994708994,\n",
       " 0.8918918918918919,\n",
       " 0.9085365853658537,\n",
       " 0.8638743455497382,\n",
       " 0.8333333333333334,\n",
       " 0.8629629629629629,\n",
       " 0.926605504587156,\n",
       " 0.896414342629482,\n",
       " 0.8805031446540881,\n",
       " 0.8597560975609756,\n",
       " 0.8392156862745098,\n",
       " 0.851145038167939,\n",
       " 0.8835616438356164,\n",
       " 0.8495575221238938,\n",
       " 0.8757062146892656,\n",
       " 0.8952380952380953,\n",
       " 0.8571428571428571,\n",
       " 0.9393939393939394,\n",
       " 0.9377777777777778,\n",
       " 0.8854489164086687,\n",
       " 0.8675,\n",
       " 0.8471615720524017,\n",
       " 0.8870056497175142,\n",
       " 0.9075144508670521,\n",
       " 0.9086021505376344,\n",
       " 0.8900255754475703,\n",
       " 0.8778004073319755,\n",
       " 0.9419795221843004,\n",
       " 0.8576388888888888,\n",
       " 0.92,\n",
       " 0.9180327868852459,\n",
       " 0.8804347826086957,\n",
       " 0.8561643835616438,\n",
       " 0.9127725856697819,\n",
       " 0.8598574821852731,\n",
       " 0.8661616161616161,\n",
       " 0.8159806295399515,\n",
       " 0.8379888268156425,\n",
       " 0.8900804289544236,\n",
       " 0.9166666666666666,\n",
       " 0.8879551820728291,\n",
       " 0.9201030927835051,\n",
       " 0.9326923076923077,\n",
       " 0.9358974358974359]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from excel file Allen\n",
    "def read_excel(fname, sname, header):\n",
    "    df = pd.read_excel(os.path.join(PATH, 'Data', fname), header=header, sheet_name=sname)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dates for sotu and save to excel\n",
    "sotu_date = read_excel('Data_Daily_1.1-2.4.xlsx', 'address', [0])\n",
    "sotu_date['sentiment'] = scores\n",
    "sotu_date = sotu_date[['Date', 'sentiment']]\n",
    "sotu_date.to_excel(os.path.join(PATH, 'Data', 'sentiment.xlsx'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
